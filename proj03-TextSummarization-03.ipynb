{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\anaconda3\\lib\\site-packages (from soynlp) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\anaconda3\\lib\\site-packages (from soynlp) (5.7.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\anaconda3\\lib\\site-packages (from soynlp) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (0.17.0)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n"
     ]
    }
   ],
   "source": [
    "# Selenium 설치.\n",
    "# !pip install selenium\n",
    "#!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import requests as rq\n",
    "import bs4\n",
    "\n",
    "import konlpy\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "\n",
    "#전처리\n",
    "def cleaning(argStr):\n",
    "    argStr = argStr.lower()\n",
    "    argStr = re.sub(r'[^가-힣0-9a-z.?!]',  ' ',argStr)\n",
    "    argStr = re.sub(\"^.*back\", \"\", argStr)\n",
    "    argStr = re.sub(r'\\s+', ' ',argStr)\n",
    "    argStr = re.sub(r'\\s$', '' ,argStr)\n",
    "    return(argStr)\n",
    "\n",
    "#뉴스 내용 크롤링\n",
    "def getLstUrlNews(argLstHl):\n",
    "    lstHyperLinkNews = []\n",
    "    for hl in argLstHl:\n",
    "        hlSubj = hl.find('strong')\n",
    "        if hlSubj:\n",
    "            strSubj = hlSubj.text\n",
    "            if (len(strSubj) > 10):\n",
    "                lstHyperLinkNews.append(hl)\n",
    "                \n",
    "    lstLinkNews = [x.get(\"href\") for x in lstHyperLinkNews]\n",
    "    return(lstLinkNews)\n",
    "\n",
    "\n",
    "def modiUrl(argUrl):    \n",
    "    if \"https://news.naver.com\" not in argUrl:\n",
    "        argUrl = \"https://news.naver.com\" + argUrl\n",
    "        \n",
    "    return(argUrl)\n",
    "\n",
    "\n",
    "def getOneTextNews(argUrl):\n",
    "    url = modiUrl(argUrl)\n",
    "    res = rq.get(url, headers=headers)\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    tagCont = soup.find(\"div\", id=\"articleBodyContents\")\n",
    "    return(cleaning(tagCont.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[' 오는 26일 2038년 아시안게임 공동 개최 유치 발표. 동반성장 결실 맺을까 관심. 5 18 현수막이 대구 광주 시청사에 동시에 내걸린 이유는... 달빛동맹으로 굳은 우의를 다져온 대구 광주가 5 18민주화운동 제41주년 기념 추모에 손을 맞잡았다.18일 대구시 광주시에 따르면 권영진 대구시장이 이날 오전 운정동 국립5 18민주묘지에서 열린 5 18 제41주년 기념식에 참석했다. 재선인 권 시장은 2014년 취임 이후 6 13 지방선거 를 눈앞에 둔 2018년을 제외하고 해마다 5 18 기념식을 위해 광주를 찾았다. 올해로 6번째다.대구시청사 외벽에는 최근 오월 시대와 눈 맞추다 세대와 발맞추다 라는 글귀가 적힌 대형 현수막이 내걸렸다. 광주시청사에 게시된 현수막과 똑같다.권 시장은 2019년 2월 당시 자유한국당 일부 의원들이 5 18 망언 을 했을 때 이용섭 광주시장에게 사과 문자를 보내기도 했다. 그는 상식 이하의 망언으로 5 18정신을 훼손하고 광주시민들에게 깊은 충격과 상처를 드렸다 며 같은 당 소속 대구시장으로서 광주시민께 충심으로 사과드린다 고 밝혔다.대구지역 5월 기념행사는 코로나19 여파에도 활발하다. 대구 5 18민중항쟁기념행사위가 지난 15일 광주 운정동 국립5 18민주묘지를 돌아보고 합동 참배와 함께 묘역을 순례했다. 대구시민도 이제 5 18을 안다는 의미의 이른바 5 18 역사기행 이다.대구시 후원을 받아 지난 10일 개막한 5 18 거리사진전 은 오는 21일까지 2 28기념중앙공원에서 열린다.18일에는 5 18 정신 계승! 역사 왜곡 폄훼 규탄 대구 시민대회 가 대구백화점 앞 동성로 민주광장에서 개최됐다.영 호남을 대표하는 두 도시는 동서화합과 국민 대통합을 위해 2013년 3월 달구벌과 빛고을의 첫 글자를 딴 달빛동맹 을 맺은 이후 돈독한 우정을 쌓고 있다.지난해부터 이어진 코로나19 사태 속에서는 부족한 병상을 서로 먼저 내주는 병상 연대 를 통해 지방자치단체 간 올바른 협력모델을 가장 먼저 제시했다.광주시 역시 2019년 대구 2 28민주화운동을 기리는 228번 시내버스를 신설한 것으로 시작으로 2 28 대한민국 최초의 민주화운동 이라는 문구를 시내버스 전면에 부착해 두 도시가 민주화에 앞장선 도시라는 정서적 연대감을 공유하고 있다.2 28민주운동은 1960년 2월 28일 대구지역 8개 고교 학생들이 부패 독재에 맞선 민주화운동이다. 4 9혁명의 도화선이 됐다는 평가를 받는다.이에 화답한 대구 도심에도 현재 5 18을 상징하는 518번 시내버스 17대가 운행 중이다. 시내버스에는 달빛동맹으로 상생 협력하는 대구 광주 평화로 하나 되는 5 18 민주화운동 2 28과 5 18로 하나 되는 대구 광주 라는 문구를 부착돼 있다.앞서 지난 2월 대구 2 28민주화운동 기념식에 다녀온 이용섭 광주시장은 오는 26일 권영진 대구시장과 함께 국회에서 2038년 제23회 아시안게임 공동개최 추진 계획을 발표할 예정이다.이 시장은 달빛내륙철도 등 사회기반시설과 아시안게임 공동 유치전 등을 통해 두 도시가 실질적 동반성장과 함께 달빛동맹의 결실을 거두게 되기 바란다 고 말했다.광주 장선욱 기자 swjang kmib.co.kr 네이버에서 국민일보를 구독하세요 클릭 국민일보 홈페이지 바로가기 치우침 없는 뉴스 국민일보 신문 구독하기 클릭 goodnews paper 국민일보 www.kmib.co.kr 무단전재 및 수집 재배포금지']\n"
     ]
    }
   ],
   "source": [
    "naverNewsUrl = \"https://news.naver.com\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "\n",
    "def getLstTextNews(argUrl, argNum=1):\n",
    "    lstNewsText =[]\n",
    "    \n",
    "    res = rq.get(argUrl, headers=headers)\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    soup01 = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    lstHyperLinkElm = soup01.find_all('a', class_=re.compile('clicks'))    \n",
    "    lstUrl = getLstUrlNews(lstHyperLinkElm)\n",
    "    \n",
    "    if  argNum==1:\n",
    "        urlTarget = lstUrl[np.random.randint(0, len(lstUrl))]\n",
    "        lstNewsText.append(getOneTextNews(urlTarget))\n",
    "    else:\n",
    "        for ith in range(argNum):\n",
    "            lstNewsText.append(getOneTextNews(lstUrl[ith]))\n",
    "        \n",
    "    return lstNewsText\n",
    "    \n",
    "lstTextNews= getLstTextNews(naverNewsUrl, 1)\n",
    "print(f\"{len(lstTextNews)}\\n{lstTextNews}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오는 26일 2038년 아시안게임 공동 개최 유치 발표. 동반성장 결실 맺을 까 관심. 5 18 현수 막이 대구 광주 시청사에 동시에 내걸린 이유는... 달빛 동맹으로 굳은 우의를 다져 온 대구 광주가 5 18 민주화운동 제 41주년 기념 추모에 손을 맞잡았다.18', '일 대구시 광주시에 따르면 권 영진 대구시장이 이날 오전 운 정동 국립 5 18 민주 묘지에서 열린 5 18 제 41주년 기념식에 참석했다.', '재선인 권 시장은 2014년 취임 이후 6 13 지방선거를 눈앞에 둔 2018년을 제외하고 해마다 5 18 기념식을 위해 광주를 찾았다.', '올해로 6 번째다.', '대구시청사 외벽에는 최근 오월 시대와 눈 맞추다 세대와 발맞추다 라는 글귀가 적힌 대형 현수막이 내걸렸다.', '광주시청사에 게시된 현수막과 똑같다.', '권 시장은 2019년 2월 당시 자유 한국당 일부 의원들이 5 18 망언 을 했을 때 이 용섭 광주시장에게 사과 문자를 보내기도 했다.', '그는 상식 이하의 망언으로 5 18 정신을 훼손하고 광주시민들에게 깊은 충격과 상처를 드렸다며 같은 당 소속 대구시장으로서 광주시민께 충심으로 사과드린다 고 밝혔다.', '대구지역 5월 기념행사는 코로나 19 여파에도 활발하다.', '대구 5 18 민중 항쟁 기념행사 위가 지난 15일 광주 운 정동 국립 5 18 민주 묘지를 돌아보고 합동 참배와 함께 묘역을 순례했다.', '대구시민도 이제 5 18을 안다는 의미의 이른바 5 18 역사 기행 이다.', '대구시 후원을 받아 지난 10일 개막한 5 18 거리 사진전 은 오는 21일까지 2 28 기념 중앙공원에서 열린다.18', '일에는 5 18 정신 계승! 역사 왜곡 폄훼 규탄 대구 시민대회 가 대구 백화점 앞 동성로 민주 광장에서 개최됐다.', '영 호남을 대표하는 두 도시는 동서 화합과 국민 대통합을 위해 2013년 3월 달구벌과 빛 고을의 첫 글자를 딴 달빛 동맹 을 맺은 이후 돈독한 우정을 쌓고 있다.', '지난해부터 이어진 코로나 19 사태 속에서는 부족한 병상을 서로 먼저 내주는 병상 연대를 통해 지방자치단체 간 올바른 협력모델을 가장 먼저 제시했다.', '광주시 역시 2019년 대구 2 28 민주화운동을 기리는 228번 시내버스를 신설한 것으로 시작으로 2 28 대한민국 최초의 민주화운동 이라는 문구를 시내버스 전면에 부착해 두 도시가 민주화에 앞장선 도시라는 정서적 연대감을 공유하고 있다.2', '28 민주운동은 1960년 2월 28일 대구지역 8개 고교 학생들이 부패 독재에 맞선 민주화운동이다.', '4 9 혁명의 도화선이 됐다는 평가를 받는다.', '이에 화답한 대구 도심에도 현재 5 18을 상징하는 518번 시내버스 17대가 운행 중이다.', '시내버스에는 달빛 동맹으로 상생 협력하는 대구 광주 평화로 하나 되는 5 18 민주화운동 2 28 과 5 18로 하나 되는 대구 광주 라는 문구를 부착돼 있다.', '앞서 지난 2월 대구 2 28 민주화운동 기념식에 다녀온 이 용섭 광주시장은 오는 26일 권 영진 대구시장과 함께 국회에서 2038년 제 23회 아시안게임 공동 개최 추진 계획을 발표할 예정이다.', '이 시장은 달빛 내륙 철도 등 사회기반시설과 아시안게임 공동 유치 전 등을 통해 두 도시가 실질적 동반성장과 함께 달빛 동맹의 결실을 거두게 되기 바란다 고 말했다.', '광주 장선 욱 기자 swjang kmib.co .kr 네이버에서 국민 일보를 구독 하세요 클릭 국민 일보 홈페이지 바로 가기 치우침 없는 뉴스 국민 일보 신문 구독 하기 클릭 goodnews paper 국민 일보 www.kmib .co .kr 무단 전재 및 수집 재배포금지']\n",
      "23\n",
      "['오는 26일 2038년 아시안게임 공동 개최 유치 발표. 동반성장 결실 맺을 까 관심. 5 18 현수 막이 대구 광주 시청사에 동시에 내걸린 이유는... 달빛 동맹으로 굳은 우의를 다져 온 대구 광주가 5 18 민주화운동 제 41주년 기념 추모에 손을 맞잡았다.18', '일 대구시 광주시에 따르면 권 영진 대구시장이 이날 오전 운 정동 국립 5 18 민주 묘지에서 열린 5 18 제 41주년 기념식에 참석했다.', '재선인 권 시장은 2014년 취임 이후 6 13 지방선거를 눈앞에 둔 2018년을 제외하고 해마다 5 18 기념식을 위해 광주를 찾았다.', '올해로 6 번째다.', '대구시청사 외벽에는 최근 오월 시대와 눈 맞추다 세대와 발맞추다 라는 글귀가 적힌 대형 현수막이 내걸렸다.', '광주시청사에 게시된 현수막과 똑같다.', '권 시장은 2019년 2월 당시 자유 한국당 일부 의원들이 5 18 망언 을 했을 때 이 용섭 광주시장에게 사과 문자를 보내기도 했다.', '그는 상식 이하의 망언으로 5 18 정신을 훼손하고 광주시민들에게 깊은 충격과 상처를 드렸다며 같은 당 소속 대구시장으로서 광주시민께 충심으로 사과드린다 고 밝혔다.', '대구지역 5월 기념행사는 코로나 19 여파에도 활발하다.', '대구 5 18 민중 항쟁 기념행사 위가 지난 15일 광주 운 정동 국립 5 18 민주 묘지를 돌아보고 합동 참배와 함께 묘역을 순례했다.', '대구시민도 이제 5 18을 안다는 의미의 이른바 5 18 역사 기행 이다.', '대구시 후원을 받아 지난 10일 개막한 5 18 거리 사진전 은 오는 21일까지 2 28 기념 중앙공원에서 열린다.18', '일에는 5 18 정신 계승! 역사 왜곡 폄훼 규탄 대구 시민대회 가 대구 백화점 앞 동성로 민주 광장에서 개최됐다.', '영 호남을 대표하는 두 도시는 동서 화합과 국민 대통합을 위해 2013년 3월 달구벌과 빛 고을의 첫 글자를 딴 달빛 동맹 을 맺은 이후 돈독한 우정을 쌓고 있다.', '지난해부터 이어진 코로나 19 사태 속에서는 부족한 병상을 서로 먼저 내주는 병상 연대를 통해 지방자치단체 간 올바른 협력모델을 가장 먼저 제시했다.', '광주시 역시 2019년 대구 2 28 민주화운동을 기리는 228번 시내버스를 신설한 것으로 시작으로 2 28 대한민국 최초의 민주화운동 이라는 문구를 시내버스 전면에 부착해 두 도시가 민주화에 앞장선 도시라는 정서적 연대감을 공유하고 있다.2', '28 민주운동은 1960년 2월 28일 대구지역 8개 고교 학생들이 부패 독재에 맞선 민주화운동이다.', '4 9 혁명의 도화선이 됐다는 평가를 받는다.', '이에 화답한 대구 도심에도 현재 5 18을 상징하는 518번 시내버스 17대가 운행 중이다.', '시내버스에는 달빛 동맹으로 상생 협력하는 대구 광주 평화로 하나 되는 5 18 민주화운동 2 28 과 5 18로 하나 되는 대구 광주 라는 문구를 부착돼 있다.', '앞서 지난 2월 대구 2 28 민주화운동 기념식에 다녀온 이 용섭 광주시장은 오는 26일 권 영진 대구시장과 함께 국회에서 2038년 제 23회 아시안게임 공동 개최 추진 계획을 발표할 예정이다.', '이 시장은 달빛 내륙 철도 등 사회기반시설과 아시안게임 공동 유치 전 등을 통해 두 도시가 실질적 동반성장과 함께 달빛 동맹의 결실을 거두게 되기 바란다 고 말했다.', '광주 장선 욱 기자 swjang kmib.co .kr 네이버에서 국민 일보를 구독 하세요 클릭 국민 일보 홈페이지 바로 가기 치우침 없는 뉴스 국민 일보 신문 구독 하기 클릭 goodnews paper 국민 일보 www.kmib .co .kr 무단 전재 및 수집 재배포금지']\n"
     ]
    }
   ],
   "source": [
    "kkma = konlpy.tag.Kkma()\n",
    "lstSent = kkma.sentences(lstTextNews[0])\n",
    "print(lstSent)\n",
    "print(f\"{len(lstSent)}\\n{lstSent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671\n",
      "[('오는', 'Verb'), ('26일', 'Number'), ('2038년', 'Number'), ('아시안', 'Noun'), ('게임', 'Noun'), ('공동', 'Noun'), ('개최', 'Noun'), ('유치', 'Noun'), ('발표', 'Noun'), ('.', 'Punctuation'), ('동반성', 'Noun'), ('장', 'Noun'), ('결실', 'Noun'), ('맺을까', 'Verb'), ('관심', 'Noun'), ('.', 'Punctuation'), ('5', 'Number'), ('18', 'Number'), ('현수막', 'Noun'), ('이', 'Josa'), ('대구', 'Noun'), ('광주', 'Noun'), ('시', 'Noun'), ('청사', 'Noun'), ('에', 'Josa'), ('동시', 'Noun'), ('에', 'Josa'), ('내', 'Noun'), ('걸린', 'Verb'), ('이유', 'Noun'), ('는', 'Josa'), ('...', 'Punctuation'), ('달빛', 'Noun'), ('동맹', 'Noun'), ('으로', 'Josa'), ('굳은', 'Adjective'), ('우의', 'Noun'), ('를', 'Josa'), ('다져', 'Verb'), ('온', 'Noun'), ('대구', 'Noun'), ('광주', 'Noun'), ('가', 'Josa'), ('5', 'Number'), ('18', 'Number'), ('민주화', 'Noun'), ('운동', 'Noun'), ('제', 'Noun'), ('41', 'Number'), ('주년', 'Noun'), ('기념', 'Noun'), ('추모', 'Noun'), ('에', 'Josa'), ('손', 'Noun'), ('을', 'Josa'), ('맞', 'Verb'), ('잡았다', 'Verb'), ('.', 'Punctuation'), ('18일', 'Number'), ('대구시', 'Noun'), ('광주시', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('권영진', 'Noun'), ('대구시', 'Noun'), ('장이', 'Suffix'), ('이', 'Determiner'), ('날', 'Noun'), ('오전', 'Noun'), ('운정동', 'Noun'), ('국립', 'Noun'), ('5', 'Number'), ('18', 'Number'), ('민주', 'Noun'), ('묘지', 'Noun'), ('에서', 'Josa'), ('열린', 'Verb'), ('5', 'Number'), ('18', 'Number'), ('제', 'Noun'), ('41', 'Number'), ('주년', 'Noun'), ('기념', 'Noun'), ('식', 'Noun'), ('에', 'Josa'), ('참석', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('재선', 'Noun'), ('인', 'Josa'), ('권', 'Noun'), ('시장', 'Noun'), ('은', 'Josa'), ('2014년', 'Number'), ('취임', 'Noun'), ('이후', 'Noun'), ('6', 'Number'), ('13', 'Number'), ('지방선거', 'Noun'), ('를', 'Noun'), ('눈앞', 'Noun'), ('에', 'Josa'), ('둔', 'Verb'), ('2018년', 'Number'), ('을', 'Foreign'), ('제외', 'Noun'), ('하고', 'Josa'), ('해마', 'Noun'), ('다', 'Josa'), ('5', 'Number'), ('18', 'Number'), ('기념', 'Noun'), ('식', 'Noun'), ('을', 'Josa'), ('위해', 'Noun'), ('광주', 'Noun'), ('를', 'Josa'), ('찾았다', 'Verb'), ('.', 'Punctuation'), ('올해', 'Noun'), ('로', 'Josa'), ('6', 'Number'), ('번', 'Noun'), ('째', 'Suffix'), ('다', 'Josa'), ('.', 'Punctuation'), ('대구시', 'Noun'), ('청사', 'Noun'), ('외벽', 'Noun'), ('에는', 'Josa'), ('최근', 'Noun'), ('오월', 'Noun'), ('시대', 'Noun'), ('와', 'Josa'), ('눈', 'Noun'), ('맞추다', 'Verb'), ('세대', 'Noun'), ('와', 'Josa'), ('발', 'Noun'), ('맞추다', 'Verb'), ('라는', 'Josa'), ('글귀', 'Noun'), ('가', 'Josa'), ('적힌', 'Verb'), ('대형', 'Noun'), ('현수막', 'Noun'), ('이', 'Josa'), ('내', 'Noun'), ('걸렸다', 'Verb'), ('.', 'Punctuation'), ('광주시', 'Noun'), ('청사', 'Noun'), ('에', 'Josa'), ('게시', 'Noun'), ('된', 'Verb'), ('현수막', 'Noun'), ('과', 'Josa'), ('똑같다', 'Adjective'), ('.', 'Punctuation'), ('권', 'Noun'), ('시장', 'Noun'), ('은', 'Josa'), ('2019년', 'Number'), ('2월', 'Number'), ('당시', 'Noun'), ('자유', 'Noun'), ('한국', 'Noun'), ('당', 'Noun'), ('일부', 'Noun'), ('의원', 'Noun'), ('들이', 'Verb'), ('5', 'Number'), ('18', 'Number'), ('망언', 'Noun'), ('을', 'Josa'), ('했을', 'Verb'), ('때', 'Noun'), ('이용섭', 'Noun'), ('광주시', 'Noun'), ('장', 'Noun'), ('에게', 'Josa'), ('사과', 'Noun'), ('문자', 'Noun'), ('를', 'Josa'), ('보', 'Noun'), ('내기', 'Noun'), ('도', 'Josa'), ('했다', 'Verb'), ('.', 'Punctuation'), ('그', 'Noun'), ('는', 'Josa'), ('상식', 'Noun'), ('이하', 'Noun'), ('의', 'Josa'), ('망언', 'Noun'), ('으로', 'Josa'), ('5', 'Number'), ('18', 'Number'), ('정신', 'Noun'), ('을', 'Josa'), ('훼손', 'Noun'), ('하고', 'Josa'), ('광주시', 'Noun'), ('민', 'Noun'), ('들', 'Suffix'), ('에게', 'Josa'), ('깊은', 'Adjective'), ('충격', 'Noun'), ('과', 'Josa'), ('상처', 'Noun'), ('를', 'Josa'), ('드렸다', 'Verb'), ('며', 'Noun'), ('같은', 'Adjective'), ('당', 'Noun'), ('소속', 'Noun'), ('대구', 'Noun'), ('시장', 'Noun'), ('으로서', 'Josa'), ('광주시', 'Noun'), ('민', 'Noun'), ('께', 'Josa'), ('충심', 'Noun'), ('으로', 'Josa'), ('사과', 'Noun'), ('드', 'Noun'), ('린다', 'Noun'), ('고', 'Noun'), ('밝혔다', 'Verb'), ('.', 'Punctuation'), ('대구', 'Noun'), ('지역', 'Noun'), ('5월', 'Number'), ('기념', 'Noun'), ('행사', 'Noun'), ('는', 'Josa'), ('코로나', 'Noun'), ('19', 'Number'), ('여파', 'Noun'), ('에도', 'Josa'), ('활발', 'Noun'), ('하다', 'Verb'), ('.', 'Punctuation'), ('대구', 'Noun'), ('5', 'Number'), ('18', 'Number'), ('민중', 'Noun'), ('항쟁', 'Noun'), ('기념', 'Noun'), ('행사', 'Noun'), ('위', 'Noun'), ('가', 'Josa'), ('지난', 'Noun'), ('15일', 'Number'), ('광주', 'Noun'), ('운정동', 'Noun'), ('국립', 'Noun'), ('5', 'Number'), ('18', 'Number'), ('민주', 'Noun'), ('묘지', 'Noun'), ('를', 'Josa'), ('돌', 'Noun'), ('아보', 'Noun'), ('고', 'Josa'), ('합동', 'Noun'), ('참배', 'Noun'), ('와', 'Josa'), ('함께', 'Adverb'), ('묘역', 'Noun'), ('을', 'Josa'), ('순례', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('대구시', 'Noun'), ('민', 'Noun'), ('도', 'Josa'), ('이제', 'Noun'), ('5', 'Number'), ('18', 'Number'), ('을', 'Josa'), ('안다는', 'Verb'), ('의미', 'Noun'), ('의', 'Josa'), ('이른바', 'Adverb'), ('5', 'Number'), ('18', 'Number'), ('역사', 'Noun'), ('기행', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation'), ('대구시', 'Noun'), ('후원', 'Noun'), ('을', 'Josa'), ('받아', 'Verb'), ('지난', 'Noun'), ('10일', 'Number'), ('개막', 'Noun'), ('한', 'Josa'), ('5', 'Number'), ('18', 'Number'), ('거리', 'Noun'), ('사진전', 'Noun'), ('은', 'Noun'), ('오는', 'Verb'), ('21일', 'Number'), ('까지', 'Noun'), ('2', 'Number'), ('28', 'Number'), ('기념', 'Noun'), ('중앙', 'Noun'), ('공원', 'Noun'), ('에서', 'Josa'), ('열', 'Noun'), ('린다', 'Noun'), ('.', 'Punctuation'), ('18일', 'Number'), ('에는', 'Josa'), ('5', 'Number'), ('18', 'Number'), ('정신', 'Noun'), ('계승', 'Noun'), ('!', 'Punctuation'), ('역사', 'Noun'), ('왜곡', 'Noun'), ('폄훼', 'Noun'), ('규탄', 'Noun'), ('대구', 'Noun'), ('시민', 'Noun'), ('대회', 'Noun'), ('가', 'Verb'), ('대구', 'Noun'), ('백화점', 'Noun'), ('앞', 'Noun'), ('동성로', 'Noun'), ('민주', 'Noun'), ('광장', 'Noun'), ('에서', 'Josa'), ('개최', 'Noun'), ('됐다', 'Verb'), ('.', 'Punctuation'), ('영', 'Noun'), ('호남', 'Noun'), ('을', 'Josa'), ('대표', 'Noun'), ('하는', 'Verb'), ('두', 'Noun'), ('도시', 'Noun'), ('는', 'Josa'), ('동서', 'Noun'), ('화합', 'Noun'), ('과', 'Josa'), ('국민', 'Noun'), ('대', 'Verb'), ('통합', 'Noun'), ('을', 'Josa'), ('위해', 'Noun'), ('2013년', 'Number'), ('3월', 'Number'), ('달구벌', 'Noun'), ('과', 'Josa'), ('빛고을', 'Noun'), ('의', 'Josa'), ('첫', 'Noun'), ('글자', 'Noun'), ('를', 'Josa'), ('딴', 'Verb'), ('달빛', 'Noun'), ('동맹', 'Noun'), ('을', 'Josa'), ('맺은', 'Verb'), ('이후', 'Noun'), ('돈독', 'Noun'), ('한', 'Josa'), ('우정', 'Noun'), ('을', 'Josa'), ('쌓고', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation'), ('지난해', 'Noun'), ('부터', 'Noun'), ('이어진', 'Noun'), ('코로나', 'Noun'), ('19', 'Number'), ('사태', 'Noun'), ('속', 'Noun'), ('에서는', 'Josa'), ('부족', 'Noun'), ('한', 'Josa'), ('병상', 'Noun'), ('을', 'Josa'), ('서로', 'Noun'), ('먼저', 'Noun'), ('내주', 'Noun'), ('는', 'Josa'), ('병상', 'Noun'), ('연대', 'Noun'), ('를', 'Noun'), ('통해', 'Noun'), ('지방자치단체', 'Noun'), ('간', 'Noun'), ('올바른', 'Adjective'), ('협력', 'Noun'), ('모델', 'Noun'), ('을', 'Josa'), ('가장', 'Noun'), ('먼저', 'Noun'), ('제시', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('광주시', 'Noun'), ('역시', 'Noun'), ('2019년', 'Number'), ('대구', 'Noun'), ('2', 'Number'), ('28', 'Number'), ('민주화', 'Noun'), ('운동', 'Noun'), ('을', 'Josa'), ('기리', 'Noun'), ('는', 'Josa'), ('228', 'Number'), ('번', 'Noun'), ('시내버스', 'Noun'), ('를', 'Josa'), ('신설', 'Noun'), ('한', 'Josa'), ('것', 'Noun'), ('으로', 'Josa'), ('시작', 'Noun'), ('으로', 'Josa'), ('2', 'Number'), ('28', 'Number'), ('대한민국', 'Noun'), ('최초', 'Noun'), ('의', 'Josa'), ('민주화', 'Noun'), ('운동', 'Noun'), ('이라는', 'Josa'), ('문구', 'Noun'), ('를', 'Josa'), ('시내버스', 'Noun'), ('전면', 'Noun'), ('에', 'Josa'), ('부착', 'Noun'), ('해', 'Josa'), ('두', 'Noun'), ('도시', 'Noun'), ('가', 'Josa'), ('민주화', 'Noun'), ('에', 'Josa'), ('앞장', 'Noun'), ('선', 'Noun'), ('도시', 'Noun'), ('라는', 'Josa'), ('정서', 'Noun'), ('적', 'Noun'), ('연', 'Noun'), ('대감', 'Noun'), ('을', 'Josa'), ('공유', 'Noun'), ('하고', 'Josa'), ('있다', 'Adjective'), ('.', 'Punctuation'), ('2', 'Number'), ('28', 'Number'), ('민주', 'Noun'), ('운동', 'Noun'), ('은', 'Josa'), ('1960년', 'Number'), ('2월', 'Number'), ('28일', 'Number'), ('대구', 'Noun'), ('지역', 'Noun'), ('8', 'Number'), ('개', 'Noun'), ('고교', 'Noun'), ('학생', 'Noun'), ('들이', 'Verb'), ('부패', 'Noun'), ('독재', 'Noun'), ('에', 'Josa'), ('맞선', 'Noun'), ('민주화', 'Noun'), ('운동', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation'), ('4', 'Number'), ('9', 'Number'), ('혁명', 'Noun'), ('의', 'Josa'), ('도화선', 'Noun'), ('이', 'Josa'), ('됐다는', 'Verb'), ('평가', 'Noun'), ('를', 'Josa'), ('받는다', 'Verb'), ('.', 'Punctuation'), ('이', 'Noun'), ('에', 'Josa'), ('화답', 'Noun'), ('한', 'Josa'), ('대구', 'Noun'), ('도심', 'Noun'), ('에도', 'Josa'), ('현재', 'Noun'), ('5', 'Number'), ('18', 'Number'), ('을', 'Josa'), ('상징', 'Noun'), ('하는', 'Verb'), ('518', 'Number'), ('번', 'Noun'), ('시내버스', 'Noun'), ('17', 'Number'), ('대가', 'Noun'), ('운행', 'Noun'), ('중이', 'Noun'), ('다', 'Josa'), ('.', 'Punctuation'), ('시내버스', 'Noun'), ('에는', 'Josa'), ('달빛', 'Noun'), ('동맹', 'Noun'), ('으로', 'Josa'), ('상생', 'Noun'), ('협력', 'Noun'), ('하는', 'Verb'), ('대구', 'Noun'), ('광주', 'Noun'), ('평화로', 'Noun'), ('하나', 'Noun'), ('되는', 'Verb'), ('5', 'Number'), ('18', 'Number'), ('민주화', 'Noun'), ('운동', 'Noun'), ('2', 'Number'), ('28', 'Number'), ('과', 'Noun'), ('5', 'Number'), ('18', 'Number'), ('로', 'Noun'), ('하나', 'Noun'), ('되는', 'Verb'), ('대구', 'Noun'), ('광주', 'Noun'), ('라는', 'Josa'), ('문구', 'Noun'), ('를', 'Josa'), ('부착', 'Noun'), ('돼', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation'), ('앞서', 'Noun'), ('지난', 'Noun'), ('2월', 'Number'), ('대구', 'Noun'), ('2', 'Number'), ('28', 'Number'), ('민주화', 'Noun'), ('운동', 'Noun'), ('기념', 'Noun'), ('식', 'Noun'), ('에', 'Josa'), ('다녀온', 'Verb'), ('이용섭', 'Noun'), ('광주시', 'Noun'), ('장', 'Noun'), ('은', 'Josa'), ('오는', 'Verb'), ('26일', 'Number'), ('권영진', 'Noun'), ('대구시', 'Noun'), ('장', 'Noun'), ('과', 'Josa'), ('함께', 'Adverb'), ('국회', 'Noun'), ('에서', 'Josa'), ('2038년', 'Number'), ('제', 'Noun'), ('23회', 'Number'), ('아시안', 'Noun'), ('게임', 'Noun'), ('공동', 'Noun'), ('개최', 'Noun'), ('추진', 'Noun'), ('계획', 'Noun'), ('을', 'Josa'), ('발표', 'Noun'), ('할', 'Verb'), ('예정', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation'), ('이', 'Noun'), ('시장', 'Noun'), ('은', 'Josa'), ('달빛', 'Noun'), ('내륙', 'Noun'), ('철도', 'Noun'), ('등', 'Noun'), ('사회', 'Noun'), ('기반시설', 'Noun'), ('과', 'Josa'), ('아시안', 'Noun'), ('게임', 'Noun'), ('공동', 'Noun'), ('유치', 'Noun'), ('전', 'Noun'), ('등', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('두', 'Noun'), ('도시', 'Noun'), ('가', 'Josa'), ('실질', 'Noun'), ('적', 'Noun'), ('동반성', 'Noun'), ('장', 'Noun'), ('과', 'Josa'), ('함께', 'Adverb'), ('달빛', 'Noun'), ('동맹', 'Noun'), ('의', 'Josa'), ('결실', 'Noun'), ('을', 'Josa'), ('거두', 'Noun'), ('게', 'Josa'), ('되기', 'Verb'), ('바란', 'Noun'), ('다', 'Josa'), ('고', 'Noun'), ('말', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('광주', 'Noun'), ('장선', 'Noun'), ('욱', 'Noun'), ('기자', 'Noun'), ('swjang', 'Alpha'), ('kmib.co.kr', 'URL'), ('네이버', 'Noun'), ('에서', 'Josa'), ('국민일보', 'Noun'), ('를', 'Josa'), ('구독', 'Noun'), ('하세요', 'Verb'), ('클릭', 'Noun'), ('국민일보', 'Noun'), ('홈페이지', 'Noun'), ('바로가기', 'Noun'), ('치우침', 'Verb'), ('없는', 'Adjective'), ('뉴스', 'Noun'), ('국민일보', 'Noun'), ('신문', 'Noun'), ('구독', 'Noun'), ('하기', 'Verb'), ('클릭', 'Noun'), ('goodnews', 'Alpha'), ('paper', 'Alpha'), ('국민일보', 'Noun'), ('www.kmib.co.kr', 'URL'), ('무단', 'Noun'), ('전', 'Modifier'), ('재', 'Noun'), ('및', 'Noun'), ('수집', 'Noun'), ('재', 'Noun'), ('배포', 'Noun'), ('금지', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter        # Okt는 작동하지 않음!\n",
    "my_twitter=Twitter()   \n",
    "lstPosSent = my_twitter.pos(lstTextNews[0])\n",
    "print(f\"{len(lstPosSent)}\\n{lstPosSent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] used default noun predictor; Sejong corpus predictor\n",
      "[Noun Extractor] used noun_predictor_sejong\n",
      "[Noun Extractor] All 2398 r features was loaded\n",
      "\r",
      "[Noun Extractor] scanning was done (L,R) has (40, 0) tokens\n",
      "\r",
      "[Noun Extractor] building L-R graph was done\n",
      "[Noun Extractor] 0 nouns are extracted\n"
     ]
    }
   ],
   "source": [
    "# from soynlp.noun import LRNounExtractor\n",
    "# noun_extractor = LRNounExtractor()\n",
    "# nouns = noun_extractor.train_extract(lstTextNews[0]) # list of str like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "[[('오', 'VV'), ('는', 'ETD'), ('26', 'NR'), ('일', 'NNM'), ('2038', 'NR'), ('년', 'NNM'), ('아시안', 'NNG'), ('게임', 'NNG'), ('공동', 'NNG'), ('개최', 'NNG'), ('유치', 'NNG'), ('발표', 'NNG'), ('.', 'SF'), ('동반', 'NNG'), ('성장', 'NNG'), ('결실', 'NNG'), ('맺', 'VV'), ('을', 'ETD'), ('까', 'VV'), ('아', 'ECS'), ('관심', 'NNG'), ('.', 'SF'), ('5', 'NR'), ('18', 'NR'), ('현수', 'NNG'), ('막이', 'NNG'), ('대구', 'NNG'), ('광주', 'NNG'), ('시청사', 'NNG'), ('에', 'JKM'), ('동시', 'NNG'), ('에', 'JKM'), ('내걸리', 'VV'), ('ㄴ', 'ETD'), ('이유', 'NNG'), ('는', 'JX'), ('...', 'SE'), ('달', 'VV'), ('ㄹ', 'ETD'), ('빛', 'NNG'), ('동맹', 'NNG'), ('으로', 'JKM'), ('굳', 'VA'), ('은', 'ETD'), ('우의', 'NNG'), ('를', 'JKO'), ('다지', 'VV'), ('어', 'ECS'), ('오', 'VV'), ('ㄴ', 'ETD'), ('대구', 'NNG'), ('광', 'NNG'), ('주가', 'NNG'), ('5', 'NR'), ('18', 'NR'), ('민주화', 'NNG'), ('운동', 'NNG'), ('제', 'NNG'), ('41', 'NR'), ('주년', 'NNM'), ('기념', 'NNG'), ('추모', 'NNG'), ('에', 'JKM'), ('손', 'NNG'), ('을', 'JKO'), ('맞잡', 'VV'), ('았', 'EPT'), ('다', 'EFN'), ('.', 'SF'), ('18', 'NR')], [('일', 'NNG'), ('대구시', 'NNG'), ('광주시', 'NNG'), ('에', 'JKM'), ('따르', 'VV'), ('면', 'ECE'), ('권', 'NNG'), ('영진', 'NNG'), ('대구', 'NNG'), ('시장', 'NNG'), ('이', 'JKS'), ('이날', 'NNG'), ('오전', 'NNG'), ('운', 'NNG'), ('정동', 'NNP'), ('국립', 'NNG'), ('5', 'NR'), ('18', 'NR'), ('민주', 'NNG'), ('묘지', 'NNG'), ('에서', 'JKM'), ('열리', 'VV'), ('ㄴ', 'ETD'), ('5', 'NR'), ('18', 'NR'), ('저', 'NP'), ('의', 'JKG'), ('41', 'NR'), ('주년', 'NNM'), ('기념식', 'NNG'), ('에', 'JKM'), ('참석', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('재선', 'NNG'), ('이', 'VCP'), ('ㄴ', 'ETD'), ('권', 'NNG'), ('시장', 'NNG'), ('은', 'JX'), ('2014', 'NR'), ('년', 'NNM'), ('취임', 'NNG'), ('이후', 'NNG'), ('6', 'NR'), ('13', 'NR'), ('지방', 'NNG'), ('선거', 'NNG'), ('를', 'JKO'), ('눈앞', 'NNG'), ('에', 'JKM'), ('두', 'VV'), ('ㄴ', 'ETD'), ('2018', 'NR'), ('년', 'NNM'), ('을', 'JKO'), ('제외', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), ('해마다', 'MAG'), ('5', 'NR'), ('18', 'NR'), ('기념식', 'NNG'), ('을', 'JKO'), ('위하', 'VV'), ('어', 'ECS'), ('광주', 'NNG'), ('를', 'JKO'), ('찾', 'VV'), ('았', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('올해', 'NNG'), ('로', 'JKM'), ('6', 'NR'), ('번째', 'NNB'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF')], [('대구시', 'NNG'), ('청사', 'NNG'), ('외벽', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('최근', 'NNG'), ('오월', 'NNG'), ('시대', 'NNG'), ('와', 'JKM'), ('눈', 'NNG'), ('맞추', 'VV'), ('다', 'ECS'), ('세대', 'NNG'), ('와', 'JKM'), ('발맞추', 'VV'), ('다', 'ECS'), ('이', 'VCP'), ('라는', 'ETD'), ('글귀', 'NNG'), ('가', 'JKS'), ('적히', 'VV'), ('ㄴ', 'ETD'), ('대형', 'NNG'), ('현수막', 'NNG'), ('이', 'JKS'), ('내걸리', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('광주시', 'NNG'), ('청사', 'NNG'), ('에', 'JKM'), ('게시', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), ('현수막', 'NNG'), ('과', 'JKM'), ('똑같', 'VA'), ('다', 'EFN'), ('.', 'SF')], [('권', 'NNG'), ('시장', 'NNG'), ('은', 'JX'), ('2019', 'NR'), ('년', 'NNM'), ('2', 'NR'), ('월', 'NNM'), ('당시', 'NNG'), ('자유', 'NNG'), ('한국', 'NNG'), ('당', 'XSN'), ('일부', 'NNG'), ('의원', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('5', 'NR'), ('18', 'NR'), ('망언', 'NNG'), ('을', 'NNG'), ('하', 'VV'), ('었', 'EPT'), ('을', 'ETD'), ('때', 'NNG'), ('이', 'MDT'), ('용섭', 'UN'), ('광주', 'NNG'), ('시장', 'NNG'), ('에게', 'JKM'), ('사과', 'NNG'), ('문자', 'NNG'), ('를', 'JKO'), ('보내', 'VV'), ('기', 'ETN'), ('도', 'JX'), ('하', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('그', 'NP'), ('는', 'JX'), ('상식', 'NNG'), ('이하', 'NNG'), ('의', 'JKG'), ('망언', 'NNG'), ('으로', 'JKM'), ('5', 'NR'), ('18', 'NR'), ('정신', 'NNG'), ('을', 'JKO'), ('훼손', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), ('광주', 'NNG'), ('시민', 'NNG'), ('들', 'XSN'), ('에게', 'JKM'), ('깊', 'VA'), ('은', 'ETD'), ('충격', 'NNG'), ('과', 'JKM'), ('상처', 'NNG'), ('를', 'JKO'), ('드리', 'VV'), ('었', 'EPT'), ('다며', 'ECE'), ('같', 'VA'), ('은', 'ETD'), ('당', 'NNG'), ('소속', 'NNG'), ('대구', 'NNG'), ('시장', 'NNG'), ('으로서', 'JKM'), ('광주', 'NNG'), ('시민', 'NNG'), ('께', 'JKM'), ('충심', 'NNG'), ('으로', 'JKM'), ('사과드리', 'VV'), ('ㄴ다', 'ECS'), ('이', 'VCP'), ('고', 'ECE'), ('밝히', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('대구', 'NNG'), ('지역', 'NNG'), ('5', 'NR'), ('월', 'NNM'), ('기념행사', 'NNG'), ('는', 'JX'), ('코로나', 'NNG'), ('19', 'NR'), ('여파', 'NNG'), ('에', 'JKM'), ('도', 'JX'), ('활발', 'XR'), ('하', 'XSA'), ('다', 'EFN'), ('.', 'SF')], [('대구', 'NNG'), ('5', 'NR'), ('18', 'NR'), ('민중', 'NNG'), ('항쟁', 'NNG'), ('기념행사', 'NNG'), ('위', 'NNG'), ('가', 'JKS'), ('지나', 'VV'), ('ㄴ', 'ETD'), ('15', 'NR'), ('일', 'NNM'), ('광주', 'NNG'), ('운', 'NNG'), ('정동', 'NNP'), ('국립', 'NNG'), ('5', 'NR'), ('18', 'NR'), ('민주', 'NNG'), ('묘지', 'NNG'), ('를', 'JKO'), ('돌아보', 'VV'), ('고', 'ECE'), ('합동', 'NNP'), ('참배', 'NNG'), ('와', 'JKM'), ('함께', 'MAG'), ('묘역', 'NNG'), ('을', 'JKO'), ('순례', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('대구', 'NNG'), ('시민', 'NNG'), ('도', 'JX'), ('이제', 'MAG'), ('5', 'NR'), ('18', 'NR'), ('을', 'JKO'), ('알', 'VV'), ('ㄴ다는', 'ETD'), ('의미', 'NNG'), ('의', 'JKG'), ('이른바', 'MAG'), ('5', 'NR'), ('18', 'NR'), ('역사', 'NNG'), ('기', 'ETN'), ('행', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF')], [('대구시', 'NNG'), ('후원', 'NNG'), ('을', 'JKO'), ('받', 'VV'), ('아', 'ECD'), ('지나', 'VV'), ('ㄴ', 'ETD'), ('10', 'NR'), ('일', 'NNM'), ('개막', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('5', 'NR'), ('18', 'NR'), ('거리', 'NNG'), ('사진전', 'NNG'), ('은', 'NNG'), ('오', 'VV'), ('는', 'ETD'), ('21', 'NR'), ('일', 'NNM'), ('까지', 'JX'), ('2', 'NR'), ('28', 'NR'), ('기념', 'NNG'), ('중앙', 'NNG'), ('공원', 'NNG'), ('에서', 'JKM'), ('열리', 'VV'), ('ㄴ다', 'EFN'), ('.', 'SF'), ('18', 'NR')], [('일', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('5', 'NR'), ('18', 'NR'), ('정신', 'NNG'), ('계승', 'NNG'), ('!', 'SF'), ('역사', 'NNG'), ('왜곡', 'NNG'), ('폄훼', 'NNG'), ('규탄', 'NNG'), ('대구', 'NNG'), ('시민', 'NNG'), ('대회', 'NNG'), ('가', 'VV'), ('아', 'ECS'), ('대구', 'NNG'), ('백화점', 'NNG'), ('앞', 'NNG'), ('동성로', 'NNG'), ('민주', 'NNG'), ('광장', 'NNG'), ('에서', 'JKM'), ('개최', 'NNG'), ('되', 'XSV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('영', 'NNG'), ('호남', 'NNG'), ('을', 'JKO'), ('대표', 'NNG'), ('하', 'XSV'), ('는', 'ETD'), ('두', 'MDN'), ('도시', 'NNG'), ('는', 'JX'), ('동서', 'NNG'), ('화합', 'NNG'), ('과', 'JC'), ('국민', 'NNG'), ('대', 'NNG'), ('통합', 'NNG'), ('을', 'JKO'), ('위하', 'VV'), ('어', 'ECS'), ('2013', 'NR'), ('년', 'NNM'), ('3', 'NR'), ('월', 'NNM'), ('달구벌', 'NNG'), ('과', 'JKM'), ('빛', 'NNG'), ('고을', 'NNG'), ('의', 'JKG'), ('첫', 'MDT'), ('글자', 'NNG'), ('를', 'JKO'), ('따', 'VV'), ('ㄴ', 'ETD'), ('달빛', 'NNG'), ('동맹', 'NNG'), ('을', 'NNG'), ('맺', 'VV'), ('은', 'ETD'), ('이후', 'NNG'), ('돈독', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('우정', 'NNG'), ('을', 'JKO'), ('쌓', 'VV'), ('고', 'ECE'), ('있', 'VXV'), ('다', 'EFN'), ('.', 'SF')], [('지난해', 'NNG'), ('부터', 'JX'), ('이어지', 'VV'), ('ㄴ', 'ETD'), ('코로나', 'NNG'), ('19', 'NR'), ('사태', 'NNG'), ('속', 'NNG'), ('에서', 'JKM'), ('는', 'JX'), ('부족', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('병상', 'NNG'), ('을', 'JKO'), ('서로', 'MAG'), ('먼저', 'MAG'), ('내주', 'VV'), ('는', 'ETD'), ('병상', 'NNG'), ('연대', 'NNG'), ('를', 'JKO'), ('통하', 'VV'), ('어', 'ECS'), ('지방', 'NNG'), ('자치', 'NNG'), ('단체', 'NNG'), ('간', 'NNB'), ('올바르', 'VA'), ('ㄴ', 'ETD'), ('협력', 'NNG'), ('모델', 'NNG'), ('을', 'JKO'), ('가장', 'MAG'), ('먼저', 'MAG'), ('제시', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('광주시', 'NNG'), ('역시', 'MAG'), ('2019', 'NR'), ('년', 'NNM'), ('대구', 'NNG'), ('2', 'NR'), ('28', 'NR'), ('민주화', 'NNG'), ('운동', 'NNG'), ('을', 'JKO'), ('기리', 'VV'), ('는', 'ETD'), ('228', 'NR'), ('번', 'NNM'), ('시내버스', 'NNG'), ('를', 'JKO'), ('신설', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('것', 'NNB'), ('으로', 'JKM'), ('시작', 'NNG'), ('으로', 'JKM'), ('2', 'NR'), ('28', 'NR'), ('대한민국', 'NNG'), ('최초', 'NNG'), ('의', 'JKG'), ('민주화', 'NNG'), ('운동', 'NNG'), ('이', 'VCP'), ('라는', 'ETD'), ('문구', 'NNG'), ('를', 'JKO'), ('시내', 'NNG'), ('버스', 'NNG'), ('전면', 'NNG'), ('에', 'JKM'), ('부착', 'NNG'), ('하', 'XSV'), ('어', 'ECS'), ('두', 'MDN'), ('도시', 'NNG'), ('가', 'JKS'), ('민주화', 'NNG'), ('에', 'JKM'), ('앞장서', 'VV'), ('ㄴ', 'ETD'), ('도시', 'NNG'), ('이', 'VCP'), ('라는', 'ETD'), ('정서적', 'NNG'), ('연대감', 'NNG'), ('을', 'JKO'), ('공유', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), ('있', 'VXV'), ('다', 'EFN'), ('.', 'SF'), ('2', 'NR')], [('28', 'NR'), ('민주', 'NNG'), ('운동', 'NNG'), ('은', 'JX'), ('1960', 'NR'), ('년', 'NNM'), ('2', 'NR'), ('월', 'NNM'), ('28', 'NR'), ('일', 'NNM'), ('대구', 'NNG'), ('지역', 'NNG'), ('8', 'NR'), ('개', 'NNM'), ('고교', 'NNG'), ('학생', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('부패', 'NNG'), ('독재', 'NNG'), ('에', 'JKM'), ('맞서', 'VV'), ('ㄴ', 'ETD'), ('민주화', 'NNG'), ('운동', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF')], [('4', 'NR'), ('9', 'NR'), ('혁명', 'NNG'), ('의', 'JKG'), ('도화선', 'NNG'), ('이', 'JKC'), ('되', 'VV'), ('었', 'EPT'), ('다는', 'ETD'), ('평가', 'NNG'), ('를', 'JKO'), ('받', 'VV'), ('는', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('이에', 'MAG'), ('화답', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('대구', 'NNG'), ('도심', 'NNG'), ('에', 'JKM'), ('도', 'JX'), ('현재', 'NNG'), ('5', 'NR'), ('18', 'NR'), ('을', 'JKO'), ('상징', 'NNG'), ('하', 'XSV'), ('는', 'ETD'), ('518', 'NR'), ('번', 'NNM'), ('시내버스', 'NNG'), ('17', 'NR'), ('대', 'NNM'), ('가', 'JKS'), ('운행', 'NNG'), ('중이', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF')], [('시내버스', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('달빛', 'NNG'), ('동맹', 'NNG'), ('으로', 'JKM'), ('상생', 'NNG'), ('협력', 'NNG'), ('하', 'XSV'), ('는', 'ETD'), ('대구', 'NNG'), ('광주', 'NNG'), ('평화', 'NNG'), ('로', 'JKM'), ('하나', 'NR'), ('되', 'VV'), ('는', 'ETD'), ('5', 'NR'), ('18', 'NR'), ('민주화', 'NNG'), ('운동', 'NNG'), ('2', 'NR'), ('28', 'NR'), ('과', 'NNG'), ('5', 'NR'), ('18', 'NR'), ('로', 'JKM'), ('하나', 'NR'), ('되', 'VV'), ('는', 'ETD'), ('대구', 'NNG'), ('광주', 'NNG'), ('이', 'VCP'), ('라는', 'ETD'), ('문구', 'NNG'), ('를', 'JKO'), ('부착', 'NNG'), ('되', 'XSV'), ('어', 'ECS'), ('있', 'VXV'), ('다', 'EFN'), ('.', 'SF')], [('앞', 'NNG'), ('서', 'JKM'), ('지나', 'VV'), ('ㄴ', 'ETD'), ('2', 'NR'), ('월', 'NNM'), ('대구', 'NNG'), ('2', 'NR'), ('28', 'NR'), ('민주화', 'NNG'), ('운동', 'NNG'), ('기념식', 'NNG'), ('에', 'JKM'), ('다녀오', 'VV'), ('ㄴ', 'ETD'), ('이', 'MDT'), ('용섭', 'UN'), ('광주', 'NNG'), ('시장', 'NNG'), ('은', 'JX'), ('오', 'VV'), ('는', 'ETD'), ('26', 'NR'), ('일', 'NNM'), ('권', 'NNG'), ('영진', 'NNG'), ('대구', 'NNG'), ('시장', 'NNG'), ('과', 'JKM'), ('함께', 'MAG'), ('국회', 'NNG'), ('에서', 'JKM'), ('2038', 'NR'), ('년', 'NNM'), ('저', 'NP'), ('의', 'JKG'), ('23', 'NR'), ('회', 'NNM'), ('아시안', 'NNG'), ('게임', 'NNG'), ('공동', 'NNG'), ('개최', 'NNG'), ('추진', 'NNG'), ('계획', 'NNG'), ('을', 'JKO'), ('발표', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('예정', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF')], [('이', 'NNG'), ('시장', 'NNG'), ('은', 'JX'), ('달빛', 'NNG'), ('내륙', 'NNG'), ('철도', 'NNG'), ('등', 'NNB'), ('사회', 'NNG'), ('기반', 'NNG'), ('시설', 'NNG'), ('과', 'JC'), ('아시안', 'NNG'), ('게임', 'NNG'), ('공동', 'NNG'), ('유치', 'NNG'), ('전', 'NNG'), ('등', 'NNB'), ('을', 'JKO'), ('통하', 'VV'), ('어', 'ECS'), ('두', 'MDN'), ('도시', 'NNG'), ('가', 'JKS'), ('실질적', 'NNG'), ('동반', 'NNG'), ('성장', 'NNG'), ('과', 'JKM'), ('함께', 'MAG'), ('달빛', 'NNG'), ('동맹', 'NNG'), ('의', 'JKG'), ('결실', 'NNG'), ('을', 'JKO'), ('거두', 'VV'), ('게', 'ECD'), ('되', 'VV'), ('기', 'ETN'), ('바라', 'VV'), ('ㄴ다', 'ECS'), ('이', 'VCP'), ('고', 'ECE'), ('말하', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('광주', 'NNG'), ('장선', 'NNG'), ('욱', 'MAG'), ('기자', 'NNG'), ('swjang', 'OL'), ('kmib', 'OL'), ('.', 'SF'), ('co', 'OL'), ('.', 'SF'), ('kr', 'OL'), ('네이버', 'NNP'), ('에서', 'JKM'), ('국민', 'NNG'), ('일보', 'NNG'), ('를', 'JKO'), ('구독', 'NNG'), ('하세', 'NNG'), ('요', 'JX'), ('클릭', 'NNG'), ('국민', 'NNG'), ('일보', 'NNG'), ('홈페이지', 'NNG'), ('바로', 'MAG'), ('가기', 'NNG'), ('치우치', 'VV'), ('ㅁ', 'ETN'), ('없', 'VA'), ('는', 'ETD'), ('뉴스', 'NNG'), ('국민', 'NNG'), ('일보', 'NNG'), ('신문', 'NNG'), ('구독', 'NNG'), ('하기', 'NNG'), ('클릭', 'NNG'), ('goodnews', 'OL'), ('paper', 'OL'), ('국민', 'NNG'), ('일보', 'NNG'), ('www', 'OL'), ('.', 'SF'), ('kmib', 'OL'), ('.', 'SF'), ('co', 'OL'), ('.', 'SF'), ('kr', 'OL'), ('무단', 'NNG'), ('전재', 'NNG'), ('및', 'MAG'), ('수집', 'NNG'), ('재', 'XPN'), ('배포', 'NNG'), ('금지', 'NNG')]]\n"
     ]
    }
   ],
   "source": [
    "lstPosSent2 = []\n",
    "for x in lstSent:\n",
    "    lstPosSent2.append(kkma.pos(x))\n",
    "\n",
    "print(f\"{len(lstPosSent2)}\\n{lstPosSent2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0a267c061176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstStrSent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mlstNormSent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeLstNormSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstPosSent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstPosStop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margPos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margMinLen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margOne\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{len(lstNormSent)}\\n{lstNormSent}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0a267c061176>\u001b[0m in \u001b[0;36mmakeLstNormSent\u001b[1;34m(argLstPosSent, argLstPosStop, argPos, argMinLen, argOne)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0moneSent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margLstPosSent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mstrOneSent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moneSent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mlstIsStop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margLstPosStop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstIsStop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0margMinLen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "lstPosStop = []\n",
    "#\n",
    "def makeLstNormSent(argLstPosSent,argLstPosStop, argPos=False, argMinLen=0, argOne=True):\n",
    "    \n",
    "    lstStrSent = []\n",
    "    \n",
    "    for oneSent in argLstPosSent:\n",
    "        strOneSent = \"\"\n",
    "        for word, pos in oneSent:\n",
    "            lstIsStop = [x in pos for x in argLstPosStop]\n",
    "            if not any(lstIsStop) and len(word) >= argMinLen:\n",
    "                if argPos==True:\n",
    "                    strOneSent += (\" \"+ word +\"-\"+pos)\n",
    "                else:\n",
    "                    strOneSent += (\" \"+ word)\n",
    "        lstStrSent.append(strOneSent)\n",
    "        \n",
    "    if argOne==True:\n",
    "        lstStrSent = [\" \".join(lstStrSent)]\n",
    "    return(lstStrSent)\n",
    "\n",
    "lstNormSent = makeLstNormSent(lstPosSent, lstPosStop, argPos=False, argMinLen=0, argOne=False)\n",
    "\n",
    "print(f\"{len(lstNormSent)}\\n{lstNormSent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-42b37bdaab65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstNGram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlstNGram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeNGram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstNormSent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{len(lstNGram)}\\n{lstNGram}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-42b37bdaab65>\u001b[0m in \u001b[0;36mmakeNGram\u001b[1;34m(argLstSent, argNum)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmakeNGram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margLstSent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margNum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mvectMy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margNum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margNum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mlstNGram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectMy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margLstSent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# ar2NGramCnt = vectMy.transform(argLstSent).toarray()        # 결과는 array of array.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# lstNGramCnt = list(ar2NGramCnt[0])                          # 단순한 리스트로 변환.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \"\"\"\n\u001b[0;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "def makeNGram(argLstSent, argNum=3):\n",
    "    vectMy = CountVectorizer(ngram_range=(argNum, argNum), analyzer=\"word\")\n",
    "    lstNGram = vectMy.fit(argLstSent).get_feature_names()\n",
    "    # ar2NGramCnt = vectMy.transform(argLstSent).toarray()        # 결과는 array of array.\n",
    "    # lstNGramCnt = list(ar2NGramCnt[0])                          # 단순한 리스트로 변환.\n",
    "    return(lstNGram)\n",
    "\n",
    "lstNGram = makeNGram(lstNormSent)\n",
    "print(f\"{len(lstNGram)}\\n{lstNGram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBaseDic(argLstNGram, argNum=3):\n",
    "    dicBase = {}\n",
    "    \n",
    "    for x in argLstNGram:\n",
    "        lstWord = nltk.word_tokenize(x)\n",
    "        preGram = ' '.join(lstWord[0:argNum-1])\n",
    "        postGram = lstWord[-1]\n",
    "        \n",
    "        if preGram not in dicBase.keys():\n",
    "            dicBase[preGram] = [postGram]\n",
    "        else:\n",
    "            dicBase[preGram] += [postGram]\n",
    "    return(dicBase)\n",
    "\n",
    "dicBase = makeBaseDic(lstNGram)\n",
    "print(f\"{len(dicBase)}\\n{dicBase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLstSeed(argDic):\n",
    "    return sorted(argDic.items(), key=lambda x : len(x[1]), reverse=True)\n",
    "\n",
    "lstSeed =makeLstSeed(dicBase)\n",
    "print(f\"{len(lstSeed)}\\n{lstSeed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predByNGram(argPreGram, argDicBase):\n",
    "    rangeIdx = len(argDicBase[argPreGram])\n",
    "    idxPic = np.random.randint(0, rangeIdx)\n",
    "    return(argDicBase[argPreGram][idxPic])\n",
    "\n",
    "predByNGram(lstSeed[0][0], dicBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSentByNGram(argLstSeed, argDicBase, argNum):    \n",
    "    cnt = 0\n",
    "    strOut = \"\"\n",
    "    while cnt < argNum:\n",
    "        preGram = argLstSeed.pop(0)[0]\n",
    "        strOut += \" \" + preGram\n",
    "        while preGram in argDicBase:\n",
    "            strOut += \" \" + predByNGram(preGram, argDicBase)\n",
    "            lstWord = nltk.word_tokenize(strOut)\n",
    "            preGram = ' '.join(lstWord[-3+1:])                   # a_nm1_gram 갱신.\n",
    "            cnt +=1\n",
    "            if cnt > argNum:\n",
    "                break\n",
    "    return(strOut)\n",
    "lstSeed1 = lstSeed[:]\n",
    "\n",
    "genSentByNGram(lstSeed1, dicBase, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보완사항\n",
    "- driveMy.get()의 안정성\n",
    "- makeNGram에서 빈도수\n",
    "- 최종 요약문 생성시 seed문 선택\n",
    "- BaseDic Key Error 처리\n",
    "- makeSent()에서 stopWord 인자로 변경\n",
    "### 추가 사항\n",
    "- WordCloud\n",
    "- WebPage제작 ?\n",
    "### 품사\n",
    "- 의존명사 NNB\n",
    "- 대명사 NP\n",
    "- 감탕사(IC)\n",
    "- 보조사(JX)\n",
    "- 접속조사(JC)\n",
    "\n",
    "- 선어말어미(EP)\n",
    "\n",
    "- SF\n",
    "\n",
    "- 동사(VV)  VXV\n",
    "- 형용사(VA) VXA\n",
    "- 일반부사(MAG)\n",
    "- 접속부사(MAC)\n",
    "- 감탕\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kkma = konlpy.tag.Kkma()\n",
    "lstPosStopWC = [\"VX\", \"VC\", \"MD\", \"MA\", \"IC\", \"JK\", \"JX\", \"JC\", \"E\", \"X\", \"S\", \"U\"]\n",
    "\n",
    "def makeWordCloud(argStr):\n",
    "    wc = WordCloud(font_path=\"c:/Windows/Fonts/malgun.ttf\", background_color='white', max_words=20)\n",
    "    wc.generate(argStr)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")                                    \n",
    "    plt.show()\n",
    "\n",
    "# 형태소 분석하여 불용 품사 및 2음절 이하 단어 제거하고\n",
    "# 나머지 형태소들로 1개의 문자열을 만듬\n",
    "lstNormSentWordCloud = makeLstNormSent(lstPosSent,\n",
    "                                       lstPosStopWC,\n",
    "                                       argPos=False,\n",
    "                                       argMinLen=2,\n",
    "                                       argOne=True)\n",
    "makeWordCloud(lstNormSentWordCloud[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makeCorpus(argLstSent):\n",
    "    lstCorpus = []\n",
    "    for aSent in argLstSent:\n",
    "        aSent = aSent.split()\n",
    "        lstCorpus.append(aSent)\n",
    "    return lstCorpus\n",
    "\n",
    "corpusRaw = makeCorpus(lstNormSent)\n",
    "print(f\"{len(corpusRaw)}\\n{corpusRaw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLenMaxSent(argLstStr):\n",
    "    lenMax = 0\n",
    "    for aStr in argLstStr:\n",
    "        lenSent = len(aStr.split())\n",
    "        if lenMax < lenSent: \n",
    "            lenMax = lenSent\n",
    "    return lenMax\n",
    "\n",
    "lenMaxSent = getLenMaxSent(lstNormSent)\n",
    "print(lenMaxSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus로 만든 사전 크기 확인.\n",
    "tokenizerMy = Tokenizer()\n",
    "tokenizerMy.fit_on_texts(corpusRaw)\n",
    "sizeVocab = len(tokenizerMy.word_index)\n",
    "print(f\"sizeVocab : {sizeVocab}\")\n",
    "\n",
    "# padding포함한 단어로 tokenize\n",
    "tokenizerMy = Tokenizer(num_words=sizeVocab+1) \n",
    "tokenizerMy.fit_on_texts(corpusRaw)\n",
    "    \n",
    "# Encoded corpus 만들기\n",
    "corpusEncoded = tokenizerMy.texts_to_sequences(corpusRaw)\n",
    "# print(corpusEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTrainData(argCorpusEncoded, argLenMaxSent):\n",
    "        # 학습용 문장 데이터를 만든다.\n",
    "    trainDataEncoded = []\n",
    "    for aSentEncoded in argCorpusEncoded:\n",
    "        for ith in range(2, len(aSentEncoded)+1):\n",
    "            aSequence = aSentEncoded[:ith]\n",
    "            trainDataEncoded.append(aSequence)\n",
    "            \n",
    "    trainDataEncodedPad = pad_sequences(trainDataEncoded,\n",
    "                                        maxlen=argLenMaxSent,\n",
    "                                        padding='pre')\n",
    "    # X와 Y로 쪼갠다.\n",
    "    XTrain    = trainDataEncodedPad[:,:-1]      # 끝에서 두번째 컬럼까지.\n",
    "    YTrainRaw = trainDataEncodedPad[:, -1]      # 마지막 컬럼.\n",
    "    \n",
    "    return XTrain, YTrainRaw\n",
    "\n",
    "XTrain, YTrainRaw = makeTrainData(corpusEncoded, lenMaxSent)\n",
    "print(XTrain)\n",
    "\n",
    "# YTrain을 one-hot encoding 해둔다.\n",
    "YTrain = to_categorical(YTrainRaw, num_classes=sizeVocab + 1)\n",
    "print(YTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRnnModel(argXTrain, argYTrain, argEpochs):\n",
    "    # Model 생성\n",
    "    numWords = sizeVocab + 1\n",
    "    numEmb = 10                                  # Embedding 차원수.\n",
    "    numHidden = 128                              # 메모리셀의 노드수.\n",
    "    modelRNN = Sequential()\n",
    "    modelRNN.add(Embedding(numWords, numEmb))\n",
    "    modelRNN.add(SimpleRNN(units=numHidden,\n",
    "                           return_sequences=False))# Sequence To Vector.\n",
    "    modelRNN.add(Dense(units=numWords, activation='softmax'))\n",
    "\n",
    "    # Optimizer 객체 생성 후 컴파일한다.\n",
    "    rateLearn = 0.001   # Hyper Parameter 설정\n",
    "    optMy = Adam(lr=rateLearn)\n",
    "    modelRNN.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=optMy,\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    # 학습.\n",
    "    eStopMy = EarlyStopping(monitor='loss',\n",
    "                            mode='min',\n",
    "                            patience=5,\n",
    "                            verbose=1)          # 조기 종료 허락!\n",
    "    \n",
    "    smryMy = modelRNN.fit(argXTrain,\n",
    "                          argYTrain,\n",
    "                          epochs=argEpochs,\n",
    "                          verbose=3,\n",
    "                          callbacks=[eStopMy])\n",
    "        \n",
    "    return modelRNN\n",
    "\n",
    "modelRNN = makeRnnModel(XTrain, YTrain, argEpochs=50)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률적으로 다음 단어를 예측해 주는 헬퍼 함수.\n",
    "def stochasticPrediction(argPreds, argTemperature=1.0):\n",
    "    predsMy = np.array(argPreds).astype('float64')\n",
    "    # \"온도\"가 낮을수록 확정적, 높을수록 확률적이 된다.\n",
    "    predsMy = np.log(predsMy)/ argTemperature\n",
    "    predsExp = np.exp(predsMy)\n",
    "    predsMy = predsExp/np.sum(predsExp)\n",
    "    # 1회 1개의 랜덤 표본만 추출한다.\n",
    "    probabilities = np.random.multinomial(n=1, pvals=predsMy, size=1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "# 예측을 위한 헬퍼 함수 버전 2.\n",
    "def genSentByRnn(argModel, argToken, argLstSeed, argLenMaxSent, argNumSeq, temperature=1.0): \n",
    "    cnt=0\n",
    "    strOut = argLstSeed.pop(0)[0]\n",
    "    \n",
    "    while cnt < argNumSeq:        \n",
    "        # 완성되가는 문장의 인터저 인코딩.\n",
    "        encoded = argToken.texts_to_sequences([strOut])[0]\n",
    "        \n",
    "        # Padding을 적용해서 X_test를 만든다.\n",
    "        XTest = pad_sequences([encoded], maxlen=argLenMaxSent-1, padding='pre')\n",
    "        print(XTest)\n",
    "        \n",
    "        # softmax를 통해서 출력된 예측은 일종의 \"multinomial\" 확률이다.\n",
    "        my_preds = argModel.predict(XTest)\n",
    "        \n",
    "        # 다음 단어의 인덱스를 예측한다.\n",
    "        idx = stochasticPrediction(my_preds[0], temperature)\n",
    "        \n",
    "        # 예측된 인덱스를 실제 단어로 변환.\n",
    "        wordPred = argToken.index_word[idx]\n",
    "        if wordPred not in [\".\",  \"?\", \"!\", \".-SF\", \"?-SF\", \"!-SF\"]:\n",
    "            # 예측된 단어를 결과 문장에 이어 붙인다.\n",
    "            strOut += \" \"  + wordPred\n",
    "        else:\n",
    "            strOut += \" \" + wordPred + \" \" + argLstSeed.pop(0)[0].split()[0]\n",
    "        cnt += 1\n",
    "        \n",
    "    return strOut\n",
    "\n",
    "\n",
    "lstSeed01 = lstSeed[:]\n",
    "#lstSeed01 = [('법사 위원장', ['문제']), ('상임 위원장', ['모두', '문제', '배분', '자리', '재배']), ('위원장 문제', ['관련', '마무리', '언급', '특히']), ('인사 청문회', ['뇌관', '물론', '부터', '진행']), ('권한 대행', ['원내', '이날', '특히']), ('대표 대행', ['반도체', '호중', '후보자']), ('서울 경제', ['기현', '무단', '폴리']), ('위원장 자리', ['배분', '야당', '요구']), ('공장 방문', ['ㄴ다는', '예정']), ('국회 에서', ['가지', '기자']), ('논의 난항', ['기현', '으로']), ('다음 절차', ['진행', '진행하']), ('대표 16', ['법사', '서울']), ('민주당 의원', ['위원장', '총회']), ('부동 산세', ['법안', '완화']), ('손실 보상법', ['부동', '입법']), ('원내 대표', ['16', '지나']) ]\n",
    "genSentByRnn(modelRNN,\n",
    "             tokenizerMy,\n",
    "             lstSeed01,\n",
    "             lenMaxSent,\n",
    "             100,\n",
    "             temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. RNN With Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpusRaw = makeCorpus(lstNormSent)\n",
    "lenMaxSent = getLenMaxSent(lstNormSent)\n",
    "print(f\"lenMaxSent : {lenMaxSent}\")\n",
    "\n",
    "\n",
    "tokenizerMy = Tokenizer()\n",
    "tokenizerMy.fit_on_texts(corpusRaw)\n",
    "sizeVocab = len(tokenizerMy.word_index)\n",
    "print(f\"sizeVocab : {sizeVocab}\")\n",
    "\n",
    "tokenizerMy = Tokenizer(num_words=sizeVocab+1) \n",
    "tokenizerMy.fit_on_texts(corpusRaw)\n",
    "corpusEncoded = tokenizerMy.texts_to_sequences(corpusRaw)\n",
    "#print(corpusEncoded)\n",
    "\n",
    "XTrain, YTrainRaw = makeTrainData(corpusEncoded, lenMaxSent)\n",
    "YTrain = to_categorical(YTrainRaw, num_classes=sizeVocab + 1)\n",
    "\n",
    "modelRNN = makeRnnModel(XTrain, YTrain, argEpochs=50)\n",
    "\n",
    "lstSeed01 = lstSeed[:]\n",
    "#lstSeed01 = [('법사 위원장', ['문제']), ('상임 위원장', ['모두', '문제', '배분', '자리', '재배']), ('위원장 문제', ['관련', '마무리', '언급', '특히']), ('인사 청문회', ['뇌관', '물론', '부터', '진행']), ('권한 대행', ['원내', '이날', '특히']), ('대표 대행', ['반도체', '호중', '후보자']), ('서울 경제', ['기현', '무단', '폴리']), ('위원장 자리', ['배분', '야당', '요구']), ('공장 방문', ['ㄴ다는', '예정']), ('국회 에서', ['가지', '기자']), ('논의 난항', ['기현', '으로']), ('다음 절차', ['진행', '진행하']), ('대표 16', ['법사', '서울']), ('민주당 의원', ['위원장', '총회']), ('부동 산세', ['법안', '완화']), ('손실 보상법', ['부동', '입법']), ('원내 대표', ['16', '지나']) ]\n",
    "genSentByRnn(modelRNN,\n",
    "             tokenizerMy,\n",
    "             lstSeed01,\n",
    "             lenMaxSent,\n",
    "             100,\n",
    "             temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RNN With Ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addClosingToSent(argLstSent):\n",
    "    lstSentAddClosing = []\n",
    "    endingPre = \". \"\n",
    "    for aSent in argLstSent:\n",
    "        aSent = endingPre + aSent\n",
    "        lstSentAddClosing.append(aSent)\n",
    "        \n",
    "        ending = aSent.split()[-1][-1]\n",
    "        if ending in [\".\", \"!\", \"?\"]:\n",
    "            endingPre = ending + \" \"\n",
    "    return lstSentAddClosing\n",
    "\n",
    "\n",
    "lstSentAddClosing = addClosingToSent(lstSent)\n",
    "# print(f\"{len(lstSentAddClosing)}\\n{lstSentAddClosing}\")\n",
    "\n",
    "lstPosSent = []\n",
    "for x in lstSentAddClosing:\n",
    "    lstPosSent.append(kkma.pos(x))\n",
    "\n",
    "lstNormSent = makeLstNormSent(lstPosSent,\n",
    "                              lstPosStop,\n",
    "                              argPos=False,\n",
    "                              argMinLen=0,\n",
    "                              argOne=False)\n",
    "\n",
    "corpusRaw = makeCorpus(lstNormSent)\n",
    "\n",
    "lenMaxSent = getLenMaxSent(lstNormSent)\n",
    "print(f\"lenMaxSent : {lenMaxSent}\")\n",
    "\n",
    "tokenizerMy = Tokenizer()\n",
    "tokenizerMy.fit_on_texts(corpusRaw)\n",
    "sizeVocab = len(tokenizerMy.word_index)\n",
    "print(f\"sizeVocab : {sizeVocab}\")\n",
    "\n",
    "tokenizerMy = Tokenizer(num_words=sizeVocab+1) \n",
    "tokenizerMy.fit_on_texts(corpusRaw)\n",
    "corpusEncoded = tokenizerMy.texts_to_sequences(corpusRaw)\n",
    "# print(corpusEncoded)\n",
    "\n",
    "XTrain, YTrainRaw = makeTrainData(corpusEncoded, lenMaxSent)\n",
    "YTrain = to_categorical(YTrainRaw, num_classes=sizeVocab + 1)\n",
    "\n",
    "modelRNN = makeRnnModel(XTrain, YTrain, argEpochs = 50)\n",
    "\n",
    "lstSeed01 = lstSeed[:]\n",
    "#lstSeed01 = [('법사 위원장', ['문제']), ('상임 위원장', ['모두', '문제', '배분', '자리', '재배']), ('위원장 문제', ['관련', '마무리', '언급', '특히']), ('인사 청문회', ['뇌관', '물론', '부터', '진행']), ('권한 대행', ['원내', '이날', '특히']), ('대표 대행', ['반도체', '호중', '후보자']), ('서울 경제', ['기현', '무단', '폴리']), ('위원장 자리', ['배분', '야당', '요구']), ('공장 방문', ['ㄴ다는', '예정']), ('국회 에서', ['가지', '기자']), ('논의 난항', ['기현', '으로']), ('다음 절차', ['진행', '진행하']), ('대표 16', ['법사', '서울']), ('민주당 의원', ['위원장', '총회']), ('부동 산세', ['법안', '완화']), ('손실 보상법', ['부동', '입법']), ('원내 대표', ['16', '지나']) ]\n",
    "genSentByRnn(modelRNN,\n",
    "             tokenizerMy,\n",
    "             lstSeed01,\n",
    "             lenMaxSent,\n",
    "             100,\n",
    "             temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. RNN with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstPosStop = []\n",
    "\n",
    "def makeNGramWithPos(argLstSent, argNum=3):\n",
    "    vectMy = CountVectorizer(ngram_range=(argNum, argNum),\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+-\\w+\\b\",\n",
    "                             analyzer=\"word\")\n",
    "    lstNGram = vectMy.fit(argLstSent).get_feature_names()\n",
    "    # ar2NGramCnt = vectMy.transform(argLstSent).toarray()        # 결과는 array of array.\n",
    "    # lstNGramCnt = list(ar2NGramCnt[0])                          # 단순한 리스트로 변환.\n",
    "    return(lstNGram)\n",
    "\n",
    "\n",
    "lstPosSent = []\n",
    "for x in lstSent:\n",
    "    lstPosSent.append(kkma.pos(x))\n",
    "# print(f\"{len(lstPosSent)}\\n{lstPosSent}\")\n",
    "\n",
    "lstNormSentPos = makeLstNormSent(lstPosSent,\n",
    "                                 lstPosStop,\n",
    "                                 argPos=True,\n",
    "                                 argMinLen=0,\n",
    "                                 argOne=False)\n",
    "# print(f\"{len(lstNormSentPos)}\\n{lstNormSentPos}\")\n",
    "\n",
    "\n",
    "\n",
    "lstNGramPos = makeNGramWithPos(lstNormSentPos)\n",
    "#print(f\"{len(lstNGramPos)}\\n{lstNGramPos}\")\n",
    "\n",
    "dicBasePos = makeBaseDic(lstNGramPos)\n",
    "# print(f\"{len(dicBasePos)}\\n{dicBasePos}\")\n",
    "\n",
    "lstSeedPos = makeLstSeed(dicBasePos)\n",
    "# print(f\"{len(lstSeedPos)}\\n{lstSeedPos}\")\n",
    "\n",
    "\n",
    "corpusPosRaw = makeCorpus(lstNormSentPos)\n",
    "# print(f\"{len(corpusPosRaw)}\\n{corpusPosRaw}\")\n",
    "\n",
    "lenMaxSent = getLenMaxSent(lstNormSentPos)\n",
    "# print(f\"{lenMaxSent}\")\n",
    "\n",
    "tokenizerMy = Tokenizer()\n",
    "tokenizerMy.fit_on_texts(corpusPosRaw)\n",
    "# print(tokenizerMy.word_index)\n",
    "\n",
    "sizeVocab = len(tokenizerMy.word_index)\n",
    "# print(f\"sizeVocab : {sizeVocab}\")\n",
    "\n",
    "tokenizerMy = Tokenizer(num_words=sizeVocab+1) \n",
    "tokenizerMy.fit_on_texts(corpusPosRaw)\n",
    "# print(tokenizerMy.word_index)\n",
    "\n",
    "corpusEncoded = tokenizerMy.texts_to_sequences(corpusPosRaw)\n",
    "# print(f\"{len(corpusEncoded)}\\n{corpusEncoded}\")\n",
    "\n",
    "XTrain, YTrainRaw = makeTrainData(corpusEncoded, lenMaxSent)\n",
    "YTrain = to_categorical(YTrainRaw, num_classes=sizeVocab + 1)\n",
    "# print(XTrain)\n",
    "# print(YTrain)\n",
    "\n",
    "modelRNN = makeRnnModel(XTrain, YTrain, argEpochs = 50)\n",
    "lstSeed01 = lstSeedPos[:]\n",
    "# print(lstSeed01)\n",
    "\n",
    "genSentByRnn(modelRNN,\n",
    "             tokenizerMy,\n",
    "             lstSeed01,\n",
    "             lenMaxSent,\n",
    "             100,\n",
    "             temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryOneNews(argStrOneNews, argNum):\n",
    "    lstResult = []\n",
    "    lstSent =[]\n",
    "    lstPosSent = []\n",
    "    lstPosStop = []\n",
    "    lstPosStopWC = [\"VX\", \"VC\", \"MD\", \"MA\", \"IC\", \"JK\", \"JX\", \"JC\", \"E\", \"X\", \"S\", \"U\"]\n",
    "#    lstPosStopRNN = [\"JK\", \"JX\", \"JC\", \"EP\"]\n",
    "    lstPosStopRNN = []\n",
    "    lstNormSent =[]\n",
    "    lstNGram =[]\n",
    "    \n",
    "    \n",
    "    # 기사를 문장 단위 list로 변형\n",
    "    lstSent = kkma.sentences(argStrOneNews)\n",
    "    \n",
    "    # 각 문장별로 현태소를 분석하여\n",
    "    # 어근으로 구성된 문장단위 형태소 분석 list 생성\n",
    "    for x in lstSent:\n",
    "        lstPosSent.append(kkma.pos(x))\n",
    "\n",
    "    #==== TriGram Generator =========================================\n",
    "\n",
    "    # 어근 기반(noramlize) 문장 단위 string list생성\n",
    "    lstNormSent = makeLstNormSent(lstPosSent, lstPosStop)\n",
    "    \n",
    "    \n",
    "    # 어근으로 기사에 대한 NGram 생성\n",
    "    lstNGram = makeNGram(lstNormSent)\n",
    "    \n",
    "    # NGram으로 다음 단어 예측을 위한 dictionary 생성\n",
    "    dicBase = makeBaseDic(lstNGram)\n",
    "    \n",
    "    # 가장 많이 사용된 dictionalry key 찾음\n",
    "    lstSeed = makeLstSeed(dicBase)\n",
    "    lstSeed01 = lstSeed[:]\n",
    "    \n",
    "    lstResult.append(genSentByNGram(lstSeed01, dicBase, argNum))\n",
    "    \n",
    "    #==== RNN Generator ============================================\n",
    "\n",
    "    # 어근 기반(noramlize) 문장 단위 string list생성(모든 길이, 문장단위)\n",
    "    lstNormSent = makeLstNormSent(lstPosSent, lstPosStopRNN, 0, False)\n",
    "    \n",
    "    # lstNormSent(기사)로 학습된 model 반환\n",
    "    modelRNN, tokenizerMy, lenMaxSent = makeRnnModel(lstNormSent)\n",
    "    \n",
    "    lstSeed01 = lstSeed[:]\n",
    "    lstResult.append(genSentByRnn(modelRNN,\n",
    "                                  tokenizerMy,\n",
    "                                  lstSeed01,\n",
    "                                  lenMaxSent,\n",
    "                                  50,\n",
    "                                  temperature=0.5))\n",
    "    #==== Word Cloud ================================================\n",
    "\n",
    "    # WordCloud용 문장 생성(명사, 동사, 길이 2이상)하여 WordCloud 출력\n",
    "    lstNormSentWordCloud = makeLstNormSent(lstPosSent, lstPosStopWC, 2)\n",
    "    makeWordCloud(lstNormSentWordCloud[0])\n",
    "    \n",
    "\n",
    "    \n",
    "    return(lstResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryOneNews(argStrOneNews, argNum):\n",
    "    lstResult = []\n",
    "    lstSent =[]\n",
    "    lstPosSent = []\n",
    "    lstPosStop = []\n",
    "    lstPosStopWC = [\"VX\", \"VC\", \"MD\", \"MA\", \"IC\", \"JK\", \"JX\", \"JC\", \"E\", \"X\", \"S\", \"U\"]\n",
    "#    lstPosStopRNN = [\"JK\", \"JX\", \"JC\", \"EP\"]\n",
    "    lstPosStopRNN = []\n",
    "    lstNormSent =[]\n",
    "    lstNGram =[]\n",
    "    \n",
    "    \n",
    "    # 기사를 문장 단위 list로 변형\n",
    "    lstSent = kkma.sentences(argStrOneNews)\n",
    "    \n",
    "    # 각 문장별로 현태소를 분석하여\n",
    "    # 어근으로 구성된 문장단위 형태소 분석 list 생성\n",
    "    for x in lstSent:\n",
    "        lstPosSent.append(kkma.pos(x))\n",
    "\n",
    "    #==== TriGram Generator =========================================\n",
    "\n",
    "    # 어근 기반(noramlize) 문장 단위 string list생성\n",
    "    lstNormSent = makeLstNormSent(lstPosSent, lstPosStop)\n",
    "    \n",
    "    \n",
    "    # 어근으로 기사에 대한 NGram 생성\n",
    "    lstNGram = makeNGram(lstNormSent)\n",
    "    \n",
    "    # NGram으로 다음 단어 예측을 위한 dictionary 생성\n",
    "    dicBase = makeBaseDic(lstNGram)\n",
    "    \n",
    "    # 가장 많이 사용된 dictionalry key 찾음\n",
    "    lstSeed = makeLstSeed(dicBase)\n",
    "    lstSeed01 = lstSeed[:]\n",
    "    \n",
    "    lstResult.append(genSentByNGram(lstSeed01, dicBase, argNum))\n",
    "    \n",
    "    #==== RNN Generator ============================================\n",
    "\n",
    "    # 어근 기반(noramlize) 문장 단위 string list생성(모든 길이, 문장단위)\n",
    "    lstNormSent = makeLstNormSent(lstPosSent, lstPosStopRNN, 0, False)\n",
    "    \n",
    "    # lstNormSent(기사)로 학습된 model 반환\n",
    "    modelRNN, tokenizerMy, lenMaxSent = makeRnnModel(lstNormSent)\n",
    "    \n",
    "    lstSeed01 = lstSeed[:]\n",
    "    lstResult.append(genSentByRnn(modelRNN,\n",
    "                                  tokenizerMy,\n",
    "                                  lstSeed01,\n",
    "                                  lenMaxSent,\n",
    "                                  50,\n",
    "                                  temperature=0.5))\n",
    "    #==== Word Cloud ================================================\n",
    "\n",
    "    # WordCloud용 문장 생성(명사, 동사, 길이 2이상)하여 WordCloud 출력\n",
    "    lstNormSentWordCloud = makeLstNormSent(lstPosSent, lstPosStopWC, 2)\n",
    "    makeWordCloud(lstNormSentWordCloud[0])\n",
    "    \n",
    "\n",
    "    \n",
    "    return(lstResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numNews = 1\n",
    "numWord = 50\n",
    "\n",
    "def summary():\n",
    "    naverNewsUrl = \"https://news.naver.com\"\n",
    "    lstSumNews = []\n",
    "    \n",
    "    lstTextNews = getLstTextNews(naverNewsUrl, numNews)\n",
    "    print(lstTextNews)\n",
    "    for elm in lstTextNews:\n",
    "        lstStr = summaryOneNews(elm, numWord)\n",
    "        for aStr in lstStr:\n",
    "            print(\"=\"*50)\n",
    "            print(aStr)\n",
    "\n",
    "summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
